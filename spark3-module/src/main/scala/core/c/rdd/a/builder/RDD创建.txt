1.在 Spark 中创建 RDD 的创建方式可以分为四种：
1) 从集合（内存）中创建 RDD
  主要提供了两个方法：parallelize 和 makeRDD
2) 从外部存储（文件）创建 RDD
  本地的文件系统，所有 Hadoop 支持的数据集。
3) 从其他 RDD 创建
  主要是通过一个 RDD 运算完后，再产生新的 RDD。详
4) 直接创建 RDD（new）
  使用 new 的方式直接构造 RDD，一般由 Spark 框架自身使用。
其中，前两种比较常用。

2.RDD的并行度和分区
  默认情况下，Spark可以将一个作业切分多个任务后，发送给Executor节点并行计算，而能够并行计算的任务数量我们称之为并行度。
这个数量可以在构建RDD时指定。注意，这里的并行执行的任务数量，并不是指的切分任务的数量。

分区数量的计算方式：
1）从内存中读取数据时的数据分区
    读取内存数据时，数据可以按照并行度的设定进行数据的分区操作
2）从文件中读取数据时的数据分区
    读取文件数据时，数据是按照Hadoop文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异。

